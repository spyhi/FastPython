{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python for Performance\n",
    "\n",
    "Python is not a fast language. Compared to other mainstream programming languages like Java, C#, Go, Javascript, C++, etc, Python is 2-10x slower on executing comparable tasks. If that's the case, why is Python so popular in applications with heavy number-crunching aspects like scientific computing, data science, and machine learning?\n",
    "\n",
    "The answer, of course, is that Python is fun and easy to program in and requires little boilerplate code compared to these other languages. But, more importantly, **it doesn't matter that Python is slow** because it can be extended using _\"Native Modules\"_ which are written in these other fast languages and can be used as though they were pure Python.\n",
    "\n",
    "Some of you may be using these modules already! NumPy and Pandas are among the most popular Python packages, and both rely on C and even FORTRAN to speed up the most peformance-critical paths of their libraries. Machine learning libraries like Tensorflow are also largely written in C and C++ for speed, but also to access low-level features in the CPU and GPU. Even Python's comprehensive standard library implements many functions in C. Native modules are an important part of how Python has remained relevant in this brave new big-data world.\n",
    "\n",
    "\n",
    "## Why is Python so Slow?\n",
    "\n",
    "Python is an interpreted language, which means the code is read directly and freely, without first compiling and optimizing programs into machine instructions. In addition to that, Python is dynamically typed and garbage collected, which means Python figures out what kind of data type variables are and manages memory for the user. All these features make things easier for the programmer, but require processing power that would otherwise get used to complete the task.\n",
    "\n",
    "\n",
    "## Demo Time!\n",
    "\n",
    "As an example of Python's speed, let's calculate a large dot product to show how dramatic the difference can be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "\n",
    "# One Hundred MILLION dimensions...muahahahaha\n",
    "v1 = np.random.rand(1, 100000000)\n",
    "v2 = np.random.rand(1, 100000000)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "dot = 0\n",
    "\n",
    "for x, y in zip(np.nditer(v1), np.nditer(v2)):\n",
    "    dot = dot + (x*y)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "python_time = end_time - start_time\n",
    "\n",
    "print(f\"That took {python_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "better_dot = sum(x * y for x, y in zip(np.nditer(v1), np.nditer(v2)))\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "better_python_time = end_time - start_time\n",
    "\n",
    "print(f\"That took {better_python_time} seconds\")\n",
    "std_multiple = python_time/better_python_time\n",
    "print(f\"\\nThe standard lib code was {std_multiple} times faster than naive Python code\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "best_dot = np.dot(v1, v2.T)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "best_python_time = end_time - start_time\n",
    "\n",
    "print(best_python_time)\n",
    "multiple = min(python_time, better_python_time)/best_python_time\n",
    "print(f\"\\nThe numpy code was {multiple} times faster than pure Python code\")\n",
    "\n",
    "print(\"\\n\\n\\n░░░░░░░░░▄▄▄▄▄\\n░░░░░░░░▀▀▀██████▄▄▄\\n░░░░░░▄▄▄▄▄░░█████████▄\\n░░░░░▀▀▀▀█████▌░▀▐▄░▀▐█\\n░░░▀▀█████▄▄░▀██████▄██ <GOTTA GO FAST!\\n░░░▀▄▄▄▄▄░░▀▀█▄▀█════█▀\\n░░░░░░░░▀▀▀▄░░▀▀███░▀░░░░░░▄▄\\n░░░░░▄███▀▀██▄████████▄░▄▀▀▀██▌\\n░░░██▀▄▄▄██▀▄███▀░▀▀████░░░░░▀█▄\\n▄▀▀▀▄██▄▀▀▌████▒▒▒▒▒▒███░░░░▌▄▄▀\\n▌░░░░▐▀████▐███▒▒▒▒▒▐██▌\\n▀▄░░▄▀░░░▀▀████▒▒▒▒▄██▀\\n░░▀▀░░░░░░▀▀█████████▀\\n░░░░░░░░▄▄██▀██████▀█\\n░░░░░░▄██▀░░░░░▀▀▀░░█\\n░░░░░▄█░░░░░░░░░░░░░▐▌\\n░▄▄▄▄█▌░░░░░░░░░░░░░░▀█▄▄▄▄▀▀▄\\n▌░░░░░▐░░░░░░░░░░░░░░░░▀▀▄▄▄▀\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Woah...What Happened?\n",
    "\n",
    "So by now you're probably saying \"Wow, Numpy is a crazy fast! ...what gives?\" You're probably going to guess what comes next: It's a native module! According to the GitHub repo half of it is written in C! Moreover, that C code uses optimizations that are only available in programming languages that are close to the metal, such as densely packed homogenously-typed arrays and CPU-level hardware-accelerated SIMD instructions such as AVX vector operations which Python doesn't have much use for in its day-to-day life as a programming language. \n",
    "\n",
    "However, peformance was never necessarily a key requirement of Python's original design! When Python was first hatched it was a bash shell script stand-in for sysadmin tasks on a niche operating system called *Amoeba*, so it was intended more as a tool to describe jobs, organize information, and glue together specialized programs. In a way, Python is still doing the same thing when it calls on native modules like Numpy--it describes work for faster code!\n",
    "\n",
    "\n",
    "## How to Write Fast Python\n",
    "\n",
    "All this implies that perhaps the way to write performant Python is to write as little Python as possible. As mentioned before, Python is designed to offload the number crunching to programs and applications that are better suited for it. For some advanced users, this might mean [making your own native modules](https://docs.python.org/3/extending/building.html) (which we can discuss in a future lesson if there is popular demand) but for most people trying to get performance out of Python, it's best to look up performance-oriented libraries and standard lib functions.\n",
    "\n",
    "Either way, the pattern will remain the same: Get your tasks onto fast, optimized code as soon as possible and keep the data there for as long as you can."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Short Tour of Some Fast Modules\n",
    "\n",
    "To get you started, we'll show some examples of popular packages that make use of native modules to perform common Python tasks.\n",
    "\n",
    "\n",
    "## Numpy\n",
    "\n",
    "`pip install numpy`\n",
    "\n",
    "As demonstrated above, Numpy is fast...really fast. It's the flagship library in the NumFocus ecosystem, which works on keeping Python relevant in the scientific computing space, which means crunching lots of numbers. Many performance-oriented Python libraries are built on Numpy.\n",
    "\n",
    "The key to Numpy's speed is vectorized data and linear algebra, which are easy to accelerate in hardware. That means that there is a bit of a learning curve to get the most of this library, but it also means you can operate on huge amounts of data hundreds of times faster than you could in pure Python.\n",
    "\n",
    "\n",
    "## Pandas\n",
    "\n",
    "`pip install pandas`\n",
    "\n",
    "Pandas brings R dataframes to Python--or basically creates a database-like structure in memory that can be queried based on rows and column projections. Pandas is a particularly great tool for importing and cleaning data from csv, json objects, and other sources before feeding them into other libraries.\n",
    "\n",
    "Though the GitHub repo says Pandas is 93% Python, don't be fooled! Pandas is built on top of Numpy and benefits from a lot of its speed optimizations, along with writing its own critical code paths in C and C++.\n",
    "\n",
    "\n",
    "## SciPy\n",
    "\n",
    "`python -m pip install --user numpy scipy matplotlib ipython jupyter pandas sympy nose`\n",
    "\n",
    "SciPy is the last NumFocus library we'll discuss here. While Numpy and Pandas are more to contain and wrangle data, SciPy is a library for doing analysis. Cumulatively, about half of it is written in low-level languages, includling about 23% in Fortran, which is the grandpappy of all scientific programming languages and is still used often in the scientific community due to its high performance and great math algorithms. If you're looking for things like fourier transforms, this is one of your best bets.\n",
    "\n",
    "## Tensorflow\n",
    "\n",
    "`pip install tensorflow`\n",
    "`pip install tensorflow-gpu`\n",
    "\n",
    "Tensorflow is probably one of the most famous machine learning frameworks out there, and was one of the tools that put Python on the map as a premiere tool for deep learning practitioners. Deep learning in particular is a famously compute-intensive technique which was only recently made feasible in the mainstream due to the advancements in GPU technology driven by the video games industry. Basically, deep learning algorithms typically use vectors, and GPUs are basically big vector accelerators.\n",
    "\n",
    "Tensorflow is 61% C++, but of particular interest is its use of the CUDA API, which allows it to offload computation to the GPU. Similar to how you want your data to spend as little time as possible in Python for speed, Tensorflow tries to let the data spend as much time as possible on the GPU for learning, since the jump between hardware components is time-intensive on the scale of computation. All this gives Python deep learning superpowers that the language couldn't have on its own."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to Evaluate Packages for Speed\n",
    "\n",
    "A good rule of thumb is to use GitHub's language percentage feature to see if a library is wired for speed. While this isn't 100% reliable as native code can still be written badly--and as seen in Pandas, a language breakdown can be decieving--seeing low-level languages designed for speed in a Python package is a good indicator the authors care about performance and have done some work to increase their package's speed. You can also check dependences to see if any of those are fast, such as is the case with Pandas, which largely uses Numpy, which itself is very fast.\n",
    "\n",
    "You can see the language breakdown of a GitHub repo by clicking the colored bar below the top-level statistics, which reveals language percentages.\n",
    "\n",
    "![Numpy Language Breakdown](img/Numpy_Lang_Breakdown.png)\n",
    "\n",
    "In general you will be looking for compiled languages, the most popular of which include C and C++, which have bindings for Python native modules. Rust is another up-and-coming fast language, and Fortran comes up occasionally due to its history as a language for scientific computing. Beyond that, there are some fast languages that work on the JVM such as Java and Scala, though those are less common in the Python ecosystem."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
